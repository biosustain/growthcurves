{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `fit_spline` mode comparison\n",
    "\n",
    "Compares `smooth` = `fast`, `slow`, `0.5` with and without OD weighting. Includes fit-trace overlays, \u03bc_max comparison plots, and runtime swarms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import inspect\n",
    "import sys\n",
    "import time\n",
    "import types\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.interpolate import BSpline\n",
    "\n",
    "src_dir = None\n",
    "for base in (Path.cwd(), Path.cwd().parent):\n",
    "    candidate = base / \"src\"\n",
    "    if (candidate / \"growthcurves\" / \"non_parametric.py\").exists():\n",
    "        src_dir = candidate\n",
    "        break\n",
    "if src_dir is None:\n",
    "    raise RuntimeError(\"Could not locate src/growthcurves/non_parametric.py\")\n",
    "\n",
    "# Force fresh load from disk to avoid stale kernel definitions.\n",
    "for name in (\"growthcurves.non_parametric\", \"growthcurves.inference\", \"growthcurves\"):\n",
    "    sys.modules.pop(name, None)\n",
    "\n",
    "pkg = types.ModuleType(\"growthcurves\")\n",
    "pkg.__path__ = [str(src_dir / \"growthcurves\")]\n",
    "sys.modules[\"growthcurves\"] = pkg\n",
    "\n",
    "inf_path = src_dir / \"growthcurves\" / \"inference.py\"\n",
    "inf_spec = importlib.util.spec_from_file_location(\"growthcurves.inference\", inf_path)\n",
    "inf_mod = importlib.util.module_from_spec(inf_spec)\n",
    "sys.modules[\"growthcurves.inference\"] = inf_mod\n",
    "inf_spec.loader.exec_module(inf_mod)\n",
    "\n",
    "np_path = src_dir / \"growthcurves\" / \"non_parametric.py\"\n",
    "np_spec = importlib.util.spec_from_file_location(\"growthcurves.non_parametric\", np_path)\n",
    "np_mod = importlib.util.module_from_spec(np_spec)\n",
    "sys.modules[\"growthcurves.non_parametric\"] = np_mod\n",
    "np_spec.loader.exec_module(np_mod)\n",
    "\n",
    "fit_spline = np_mod.fit_spline\n",
    "print(\"Loaded fit_spline signature:\", inspect.signature(fit_spline))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_paths = [\n",
    "    Path(\"exploratory notebooks/generated_data/spline_eval_curves.csv\"),\n",
    "    Path(\"exploratory notebooks/exploratory notebooks/generated_data/spline_eval_curves.csv\"),\n",
    "]\n",
    "csv_path = next((p for p in candidate_paths if p.exists()), None)\n",
    "if csv_path is None:\n",
    "    raise RuntimeError(\"Missing generated CSV. Run spline_test_data_generation.ipynb first.\")\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "curves = []\n",
    "for curve_id, g in df.groupby(\"curve_id\", sort=False):\n",
    "    g = g.sort_values(\"t\")\n",
    "    t = g[\"t\"].to_numpy(dtype=float)\n",
    "    N_obs = g[\"N_obs\"].to_numpy(dtype=float)\n",
    "    N_true = g[\"N_true\"].to_numpy(dtype=float) if \"N_true\" in g.columns else np.full_like(t, np.nan, dtype=float)\n",
    "    mu_true = float(g[\"mu_true\"].iloc[0])\n",
    "\n",
    "    mask = np.isfinite(t) & np.isfinite(N_obs) & (N_obs > 0)\n",
    "    t, N_obs, N_true = t[mask], N_obs[mask], N_true[mask]\n",
    "    if len(t) >= 10 and np.ptp(t) > 0 and np.isfinite(mu_true) and mu_true > 0:\n",
    "        curves.append(\n",
    "            {\n",
    "                \"curve_id\": str(curve_id),\n",
    "                \"t\": t,\n",
    "                \"N_obs\": N_obs,\n",
    "                \"N_true\": N_true,\n",
    "                \"mu_true\": mu_true,\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(f\"Loaded {len(curves)} valid curves from {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_specs = [\n",
    "    {\"smooth\": \"fast\", \"use_weights\": True, \"label\": \"fast + weights\", \"color\": \"#1f77b4\"},\n",
    "    {\"smooth\": \"fast\", \"use_weights\": False, \"label\": \"fast\", \"color\": \"#17becf\"},\n",
    "    {\"smooth\": \"slow\", \"use_weights\": True, \"label\": \"slow + weights\", \"color\": \"#2ca02c\"},\n",
    "    {\"smooth\": \"slow\", \"use_weights\": False, \"label\": \"slow\", \"color\": \"#98df8a\"},\n",
    "    {\"smooth\": 0.5, \"use_weights\": True, \"label\": \"0.5 + weights\", \"color\": \"#d62728\"},\n",
    "    {\"smooth\": 0.5, \"use_weights\": False, \"label\": \"0.5\", \"color\": \"#ff9896\"},\n",
    "]\n",
    "for spec in method_specs:\n",
    "    spec[\"key\"] = f\"{spec['smooth']}|w={int(spec['use_weights'])}\"\n",
    "\n",
    "runtime_records = []\n",
    "fit_records = []\n",
    "for rec in curves:\n",
    "    t = rec[\"t\"]\n",
    "    N_obs = rec[\"N_obs\"]\n",
    "    mu_true = rec[\"mu_true\"]\n",
    "    cid = rec[\"curve_id\"]\n",
    "\n",
    "    for spec in method_specs:\n",
    "        t0 = time.perf_counter()\n",
    "        fit = fit_spline(t, N_obs, smooth=spec[\"smooth\"], use_weights=spec[\"use_weights\"])\n",
    "        dt_ms = (time.perf_counter() - t0) * 1000.0\n",
    "        runtime_records.append({\"curve_id\": cid, \"method_key\": spec[\"key\"], \"method\": spec[\"label\"], \"time_ms\": dt_ms})\n",
    "\n",
    "        if fit is None:\n",
    "            continue\n",
    "        params = fit.get(\"params\", {})\n",
    "        mu_hat = float(params.get(\"mu_max\", np.nan))\n",
    "        if not np.isfinite(mu_hat):\n",
    "            continue\n",
    "\n",
    "        fit_records.append(\n",
    "            {\n",
    "                \"curve_id\": cid,\n",
    "                \"method_key\": spec[\"key\"],\n",
    "                \"method\": spec[\"label\"],\n",
    "                \"smooth\": str(spec[\"smooth\"]),\n",
    "                \"use_weights\": bool(spec[\"use_weights\"]),\n",
    "                \"mu_true\": float(mu_true),\n",
    "                \"mu_hat\": float(mu_hat),\n",
    "                \"rel_err\": float(abs(mu_hat - mu_true) / mu_true),\n",
    "                \"spline_s\": float(params.get(\"spline_s\", np.nan)),\n",
    "            }\n",
    "        )\n",
    "\n",
    "runtime_df = pd.DataFrame(runtime_records)\n",
    "eval_df = pd.DataFrame(fit_records)\n",
    "\n",
    "rows = []\n",
    "n_total = len(curves)\n",
    "for spec in method_specs:\n",
    "    key = spec[\"key\"]\n",
    "    sub_eval = eval_df[eval_df[\"method_key\"] == key]\n",
    "    sub_time = runtime_df[runtime_df[\"method_key\"] == key][\"time_ms\"].to_numpy(dtype=float)\n",
    "    n_ok = len(sub_eval)\n",
    "    n_failed = max(n_total - n_ok, 0)\n",
    "    rel = sub_eval[\"rel_err\"].to_numpy(dtype=float)\n",
    "    rows.append(\n",
    "        {\n",
    "            \"method\": spec[\"label\"],\n",
    "            \"smooth\": str(spec[\"smooth\"]),\n",
    "            \"use_weights\": bool(spec[\"use_weights\"]),\n",
    "            \"n_curves\": int(n_total),\n",
    "            \"n_failed\": int(n_failed),\n",
    "            \"failure_rate_%\": 100.0 * n_failed / n_total if n_total else np.nan,\n",
    "            \"median_rel_err_%\": 100.0 * float(np.median(rel)) if len(rel) else np.nan,\n",
    "            \"mean_rel_err_%\": 100.0 * float(np.mean(rel)) if len(rel) else np.nan,\n",
    "            \"median_time_ms\": float(np.median(sub_time)) if len(sub_time) else np.nan,\n",
    "            \"mean_time_ms\": float(np.mean(sub_time)) if len(sub_time) else np.nan,\n",
    "        }\n",
    "    )\n",
    "\n",
    "results = pd.DataFrame(rows)\n",
    "method_order = [spec[\"label\"] for spec in method_specs]\n",
    "results[\"method\"] = pd.Categorical(results[\"method\"], categories=method_order, ordered=True)\n",
    "results = results.sort_values(\"method\").reset_index(drop=True)\n",
    "\n",
    "mu_sorted_idx = np.argsort([rec[\"mu_true\"] for rec in curves]) if curves else np.array([], dtype=int)\n",
    "example_indices = sorted(set([0, len(mu_sorted_idx) // 2, max(len(mu_sorted_idx) - 1, 0)])) if len(mu_sorted_idx) else []\n",
    "example_curves = [curves[int(mu_sorted_idx[i])] for i in example_indices if len(mu_sorted_idx)]\n",
    "\n",
    "results.round(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not example_curves:\n",
    "    raise RuntimeError(\"No example curves available to plot.\")\n",
    "\n",
    "label_to_color = {spec[\"label\"]: spec[\"color\"] for spec in method_specs}\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=len(example_curves),\n",
    "    subplot_titles=[f\"{rec['curve_id']}<br>\u03bc_true={rec['mu_true']:.3f}\" for rec in example_curves],\n",
    "    horizontal_spacing=0.04,\n",
    ")\n",
    "\n",
    "for cidx, rec in enumerate(example_curves, start=1):\n",
    "    t = rec[\"t\"]\n",
    "    N_obs = rec[\"N_obs\"]\n",
    "    N_true = rec[\"N_true\"]\n",
    "    t_eval = np.linspace(float(np.min(t)), float(np.max(t)), 300)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=t, y=N_obs, mode=\"markers\", name=\"Observed\", marker=dict(size=5, color=\"#7f7f7f\"), showlegend=(cidx == 1)),\n",
    "        row=1, col=cidx\n",
    "    )\n",
    "\n",
    "    if np.any(np.isfinite(N_true)):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=t, y=N_true, mode=\"lines\", name=\"True\", line=dict(color=\"#111111\", width=2), showlegend=(cidx == 1)),\n",
    "            row=1, col=cidx\n",
    "        )\n",
    "\n",
    "    for spec in method_specs:\n",
    "        fit = fit_spline(t, N_obs, smooth=spec[\"smooth\"], use_weights=spec[\"use_weights\"])\n",
    "        if fit is None:\n",
    "            continue\n",
    "        p = fit.get(\"params\", {})\n",
    "        if not {\"tck_t\", \"tck_c\", \"tck_k\"}.issubset(p.keys()):\n",
    "            continue\n",
    "        spline = BSpline(np.asarray(p[\"tck_t\"], dtype=float), np.asarray(p[\"tck_c\"], dtype=float), int(p[\"tck_k\"]))\n",
    "        N_fit = np.exp(np.asarray(spline(t_eval), dtype=float))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=t_eval,\n",
    "                y=N_fit,\n",
    "                mode=\"lines\",\n",
    "                name=spec[\"label\"],\n",
    "                line=dict(width=2, color=spec[\"color\"]),\n",
    "                showlegend=(cidx == 1),\n",
    "            ),\n",
    "            row=1, col=cidx\n",
    "        )\n",
    "\n",
    "fig.update_layout(template=\"plotly_white\", height=420, width=1700, title=\"Example curve fits by method\")\n",
    "for cidx in range(1, len(example_curves) + 1):\n",
    "    fig.update_xaxes(title_text=\"Time\", row=1, col=cidx)\n",
    "    fig.update_yaxes(title_text=\"OD\", row=1, col=cidx)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if eval_df.empty:\n",
    "    raise RuntimeError(\"No successful fits available for comparison plots.\")\n",
    "\n",
    "method_order = [spec[\"label\"] for spec in method_specs]\n",
    "label_to_color = {spec[\"label\"]: spec[\"color\"] for spec in method_specs}\n",
    "baseline_method = method_specs[0][\"label\"]\n",
    "\n",
    "fig_cmp = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[\n",
    "        \"Parity: \u03bc_true vs \u03bc_hat\",\n",
    "        \"Mean relative error (95% CI)\",\n",
    "        f\"Paired errors vs baseline ({baseline_method})\",\n",
    "        \"Smoothing value (spline_s) distribution\",\n",
    "    ],\n",
    "    horizontal_spacing=0.12,\n",
    "    vertical_spacing=0.18,\n",
    ")\n",
    "\n",
    "# 1) Parity plot\n",
    "for method in method_order:\n",
    "    sub = eval_df[eval_df[\"method\"] == method]\n",
    "    if sub.empty:\n",
    "        continue\n",
    "    fig_cmp.add_trace(\n",
    "        go.Scatter(\n",
    "            x=sub[\"mu_true\"], y=sub[\"mu_hat\"], mode=\"markers\",\n",
    "            name=method, marker=dict(size=5, color=label_to_color[method], opacity=0.5),\n",
    "            showlegend=True,\n",
    "        ),\n",
    "        row=1, col=1,\n",
    "    )\n",
    "if len(eval_df):\n",
    "    lo = float(min(eval_df[\"mu_true\"].min(), eval_df[\"mu_hat\"].min()))\n",
    "    hi = float(max(eval_df[\"mu_true\"].max(), eval_df[\"mu_hat\"].max()))\n",
    "    fig_cmp.add_trace(\n",
    "        go.Scatter(x=[lo, hi], y=[lo, hi], mode=\"lines\", line=dict(color=\"black\", dash=\"dash\"), name=\"Parity\", showlegend=False),\n",
    "        row=1, col=1,\n",
    "    )\n",
    "\n",
    "# 2) Mean relative error bars\n",
    "bar_x, bar_y, bar_err, bar_color = [], [], [], []\n",
    "for method in method_order:\n",
    "    vals = eval_df.loc[eval_df[\"method\"] == method, \"rel_err\"].to_numpy(dtype=float)\n",
    "    vals = vals[np.isfinite(vals)]\n",
    "    if len(vals) == 0:\n",
    "        bar_x.append(method); bar_y.append(np.nan); bar_err.append(0.0); bar_color.append(label_to_color[method])\n",
    "        continue\n",
    "    mean = float(np.mean(vals) * 100.0)\n",
    "    ci95 = float(1.96 * np.std(vals, ddof=1) / np.sqrt(len(vals)) * 100.0) if len(vals) > 1 else 0.0\n",
    "    bar_x.append(method); bar_y.append(mean); bar_err.append(ci95); bar_color.append(label_to_color[method])\n",
    "fig_cmp.add_trace(\n",
    "    go.Bar(x=bar_x, y=bar_y, error_y=dict(type=\"data\", array=bar_err, visible=True), marker=dict(color=bar_color), showlegend=False),\n",
    "    row=1, col=2,\n",
    ")\n",
    "\n",
    "# 3) Paired error scatter vs baseline\n",
    "base = eval_df[eval_df[\"method\"] == baseline_method][[\"curve_id\", \"rel_err\"]].rename(columns={\"rel_err\": \"base_rel\"})\n",
    "for method in method_order[1:]:\n",
    "    sub = eval_df[eval_df[\"method\"] == method][[\"curve_id\", \"rel_err\"]].rename(columns={\"rel_err\": \"method_rel\"})\n",
    "    merged = base.merge(sub, on=\"curve_id\", how=\"inner\")\n",
    "    if merged.empty:\n",
    "        continue\n",
    "    fig_cmp.add_trace(\n",
    "        go.Scatter(\n",
    "            x=100.0 * merged[\"base_rel\"], y=100.0 * merged[\"method_rel\"], mode=\"markers\",\n",
    "            marker=dict(size=5, color=label_to_color[method], opacity=0.5), name=method, showlegend=False,\n",
    "        ),\n",
    "        row=2, col=1,\n",
    "    )\n",
    "pair_max = float(np.nanmax(100.0 * eval_df[\"rel_err\"])) if len(eval_df) else 1.0\n",
    "pair_max = max(pair_max, 1.0)\n",
    "fig_cmp.add_trace(\n",
    "    go.Scatter(x=[0, pair_max], y=[0, pair_max], mode=\"lines\", line=dict(color=\"black\", dash=\"dash\"), showlegend=False),\n",
    "    row=2, col=1,\n",
    ")\n",
    "\n",
    "# 4) spline_s swarm\n",
    "rng = np.random.default_rng(20260224)\n",
    "for i, method in enumerate(method_order):\n",
    "    svals = eval_df.loc[eval_df[\"method\"] == method, \"spline_s\"].to_numpy(dtype=float)\n",
    "    svals = svals[np.isfinite(svals)]\n",
    "    if len(svals) == 0:\n",
    "        continue\n",
    "    xj = rng.normal(loc=float(i), scale=0.07, size=len(svals))\n",
    "    fig_cmp.add_trace(\n",
    "        go.Scatter(x=xj, y=svals, mode=\"markers\", marker=dict(size=4, color=label_to_color[method], opacity=0.45), showlegend=False),\n",
    "        row=2, col=2,\n",
    "    )\n",
    "    fig_cmp.add_trace(\n",
    "        go.Scatter(x=[float(i)], y=[float(np.mean(svals))], mode=\"markers\", marker=dict(size=9, symbol=\"x\", color=label_to_color[method]), showlegend=False),\n",
    "        row=2, col=2,\n",
    "    )\n",
    "\n",
    "fig_cmp.update_layout(template=\"plotly_white\", width=1400, height=950, title=\"\u03bc_max comparison across spline modes\")\n",
    "fig_cmp.update_xaxes(title_text=\"\u03bc_true\", row=1, col=1)\n",
    "fig_cmp.update_yaxes(title_text=\"\u03bc_hat\", row=1, col=1)\n",
    "fig_cmp.update_xaxes(title_text=\"Method\", row=1, col=2)\n",
    "fig_cmp.update_yaxes(title_text=\"Relative error (%)\", row=1, col=2)\n",
    "fig_cmp.update_xaxes(title_text=f\"{baseline_method} relative error (%)\", row=2, col=1)\n",
    "fig_cmp.update_yaxes(title_text=\"Method relative error (%)\", row=2, col=1)\n",
    "fig_cmp.update_xaxes(\n",
    "    title_text=\"Method\", row=2, col=2,\n",
    "    tickmode=\"array\",\n",
    "    tickvals=[float(i) for i in range(len(method_order))],\n",
    "    ticktext=method_order,\n",
    "    range=[-0.6, len(method_order) - 0.4],\n",
    ")\n",
    "fig_cmp.update_yaxes(title_text=\"spline_s\", row=2, col=2)\n",
    "fig_cmp.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runtime_df.empty:\n",
    "    raise RuntimeError(\"No runtime data available.\")\n",
    "\n",
    "method_order = [spec[\"label\"] for spec in method_specs]\n",
    "label_to_color = {spec[\"label\"]: spec[\"color\"] for spec in method_specs}\n",
    "fig_rt = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=[\"Mean runtime per fit (95% CI)\", \"Runtime swarm per fit\"],\n",
    "    horizontal_spacing=0.14,\n",
    ")\n",
    "\n",
    "bar_x, bar_y, bar_err, bar_color = [], [], [], []\n",
    "for method in method_order:\n",
    "    vals = runtime_df.loc[runtime_df[\"method\"] == method, \"time_ms\"].to_numpy(dtype=float)\n",
    "    vals = vals[np.isfinite(vals)]\n",
    "    mean = float(np.mean(vals)) if len(vals) else np.nan\n",
    "    ci95 = float(1.96 * np.std(vals, ddof=1) / np.sqrt(len(vals))) if len(vals) > 1 else 0.0\n",
    "    bar_x.append(method); bar_y.append(mean); bar_err.append(ci95); bar_color.append(label_to_color[method])\n",
    "fig_rt.add_trace(\n",
    "    go.Bar(x=bar_x, y=bar_y, error_y=dict(type=\"data\", array=bar_err, visible=True), marker=dict(color=bar_color), showlegend=False),\n",
    "    row=1, col=1,\n",
    ")\n",
    "\n",
    "rng = np.random.default_rng(20260224)\n",
    "for i, method in enumerate(method_order):\n",
    "    vals = runtime_df.loc[runtime_df[\"method\"] == method, \"time_ms\"].to_numpy(dtype=float)\n",
    "    vals = vals[np.isfinite(vals)]\n",
    "    if len(vals) == 0:\n",
    "        continue\n",
    "    xj = rng.normal(loc=float(i), scale=0.07, size=len(vals))\n",
    "    fig_rt.add_trace(\n",
    "        go.Scatter(x=xj, y=vals, mode=\"markers\", marker=dict(size=4, color=label_to_color[method], opacity=0.35), showlegend=False),\n",
    "        row=1, col=2,\n",
    "    )\n",
    "    fig_rt.add_trace(\n",
    "        go.Scatter(x=[float(i)], y=[float(np.median(vals))], mode=\"markers\", marker=dict(size=10, symbol=\"x\", color=label_to_color[method]), showlegend=False),\n",
    "        row=1, col=2,\n",
    "    )\n",
    "\n",
    "fig_rt.update_layout(template=\"plotly_white\", width=1450, height=520, title=\"Runtime comparison across spline modes\")\n",
    "fig_rt.update_xaxes(title_text=\"Method\", row=1, col=1)\n",
    "fig_rt.update_yaxes(title_text=\"Runtime (ms)\", row=1, col=1)\n",
    "fig_rt.update_xaxes(\n",
    "    title_text=\"Method\", row=1, col=2,\n",
    "    tickmode=\"array\",\n",
    "    tickvals=[float(i) for i in range(len(method_order))],\n",
    "    ticktext=method_order,\n",
    "    range=[-0.6, len(method_order) - 0.4],\n",
    ")\n",
    "fig_rt.update_yaxes(title_text=\"Runtime (ms)\", row=1, col=2, type=\"log\")\n",
    "fig_rt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "growthcurves_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}